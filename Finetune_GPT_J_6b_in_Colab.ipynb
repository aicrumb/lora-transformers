{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b652a7a69384d15bfc8285ecf68addd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8027e9de49554ebb939593ad0592b52d",
              "IPY_MODEL_26a4a29605844e3a95ad8ad170e01900",
              "IPY_MODEL_39fb0b40a7c84689bd01b37a868c4cb5"
            ],
            "layout": "IPY_MODEL_cda454cf669e4d6a85bd58aabe635b54"
          }
        },
        "8027e9de49554ebb939593ad0592b52d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acdbcfad927548348fae27e7b9202a2f",
            "placeholder": "​",
            "style": "IPY_MODEL_89c8fb4a1cd24de199a231aebc933c13",
            "value": " 79%"
          }
        },
        "26a4a29605844e3a95ad8ad170e01900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_def8b455a4e6425d8d3ee70c8b0ec923",
            "max": 4096,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_030f05a0531b4143a73ff8e550cea9fd",
            "value": 3253
          }
        },
        "39fb0b40a7c84689bd01b37a868c4cb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17d7a799ced64f9e93d46d3d2de74cdc",
            "placeholder": "​",
            "style": "IPY_MODEL_2e62f721674042aa986d3e95a03a34ea",
            "value": " 3253/4096 [32:05&lt;08:16,  1.70it/s]"
          }
        },
        "cda454cf669e4d6a85bd58aabe635b54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acdbcfad927548348fae27e7b9202a2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89c8fb4a1cd24de199a231aebc933c13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "def8b455a4e6425d8d3ee70c8b0ec923": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "030f05a0531b4143a73ff8e550cea9fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17d7a799ced64f9e93d46d3d2de74cdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e62f721674042aa986d3e95a03a34ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers 8bit + LoRA Finetune\n",
        "Adapted from https://colab.research.google.com/drive/1ft6wQU0BhqG5PRlwgaZJv2VukKKjU4E by AICrumb, to stay up to date with current methods (load_in_8bit). This finetunes low rank adapters for GPT2-XL (1.5B) on the Codeparrot dataset from the Transformers Book. ([transformers-book/codeparrot-train](https://huggingface.co/datasets/transformersbook/codeparrot-train))\n",
        "\n",
        "With Low Rank Adapters (LoRA) we can target just the model residual ΔW as opposed to W with finetuning, with trainable rank decomposition matrices. \n",
        "> \"LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency.\"\n",
        "\n",
        "\\- https://arxiv.org/abs/2106.09685"
      ],
      "metadata": {
        "id": "HvBUvOwqfnCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Install required libraries\n",
        "!pip install transformers ftfy sentencepiece bitsandbytes accelerate datasets -qq\n",
        "!pip install git+https://github.com/aicrumb/lora-transformers -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "wXcdA_Ufb2f9",
        "outputId": "f4090609-0c9c-4024-afdb-10174aba91c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Discarding git+https://github.com/aicrumb/lora-transformers. Command errored out with exit status 128: git clone -q https://github.com/aicrumb/lora-transformers /tmp/pip-req-build-0jyvm7mt Check the logs for full command output.\u001b[0m\n",
            "\u001b[31mERROR: Command errored out with exit status 128: git clone -q https://github.com/aicrumb/lora-transformers /tmp/pip-req-build-0jyvm7mt Check the logs for full command output.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZljHSa4AZ8f_"
      },
      "outputs": [],
      "source": [
        "from lora import replace_all_matching_layers, save_lora_layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the base model\n",
        "import os\n",
        "import transformers\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if not os.path.exists(\"offload_gpt\"):\n",
        "    os.mkdir(\"offload_gpt\")\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
        "gpt = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    \"gpt2-xl\", \n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\", \n",
        "    offload_folder=\"offload_gpt\", \n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# test generation\n",
        "prompt = tokenizer(\"A cat sat on a mat\", return_tensors='pt')\n",
        "prompt = {key: value.to(device) for key, value in prompt.items()}\n",
        "with torch.no_grad():\n",
        "    out = gpt.generate(**prompt, min_length=64, max_length=64, do_sample=True)\n",
        "tokenizer.decode(out[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "sW9jHRMBcMb3",
        "outputId": "6ce247bb-804f-4c1f-f34a-be0ed130f49e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A cat sat on a mat covered in feces. It had been left there to defecate, and he had walked on it several times before. The white cat's stomach was as big as a person's thigh, and its paws were as fat as its head, hersing blood intermittently. In the middle of\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset and optimizer, add LoRAs\n",
        "from datasets import load_dataset\n",
        "from bitsandbytes.optim import Adam8bit\n",
        "\n",
        "matches = [\"attn\"] # all the types of modules to add adapters to\n",
        "\n",
        "replace_all_matching_layers(gpt, r=16, matches=matches)\n",
        "\n",
        "for name, param in gpt.named_parameters():\n",
        "    if True in [match in name for match in matches]:\n",
        "        param.requires_grad_(True)\n",
        "    else:\n",
        "        param.requires_grad_(False)\n",
        "\n",
        "codeparrot = load_dataset(\"transformersbook/codeparrot-train\", streaming=True)\n",
        "optimizer = Adam8bit(gpt.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEjx9rt5d4Z5",
        "outputId": "ed2707e1-aa38-4694-bbd4-d9c8b4eb24de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration transformersbook--codeparrot-train-ba60c789679753de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This training loop is just a proof of concept - to show that even in the heaviest case, it still fits on a gpu.\n",
        "Depending on your finetuning task, you'll need to remove or add some parts."
      ],
      "metadata": {
        "id": "n7GqlVjieysF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train!\n",
        "import torch.nn.functional as F\n",
        "\n",
        "accumulate = 8\n",
        "max_samples = 4096\n",
        "ctx_length = 640\n",
        "\n",
        "print(\"Training on\", ctx_length*max_samples, \"new tokens...\")\n",
        "print(max_samples//accumulate, \"steps in total will be taken.\")\n",
        "\n",
        "losses = [] # for plotting later\n",
        "with torch.cuda.amp.autocast():\n",
        "    for i, row in enumerate(tqdm(codeparrot[\"train\"].take(max_samples), total=max_samples)):\n",
        "        if len(row[\"content\"]) <= 1:\n",
        "            continue\n",
        "\n",
        "        batch = tokenizer(row[\"content\"], truncation=True, max_length=ctx_length, return_tensors='pt')\n",
        "        batch = {k: v.cuda() for k, v in batch.items()}\n",
        "\n",
        "        out = gpt.forward(**batch,)\n",
        "\n",
        "        loss = F.cross_entropy(out.logits[:, :-1, :].flatten(0, -2), batch['input_ids'][:, 1:].flatten(),\n",
        "                               reduction='mean') / accumulate\n",
        "        loss.backward()\n",
        "        losses.append(loss.item() * accumulate)\n",
        "        if (i+1) % accumulate == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            print(f\"Loss at step {(i+1)//accumulate}:\", sum(losses[-accumulate:]) / accumulate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0b652a7a69384d15bfc8285ecf68addd",
            "8027e9de49554ebb939593ad0592b52d",
            "26a4a29605844e3a95ad8ad170e01900",
            "39fb0b40a7c84689bd01b37a868c4cb5",
            "cda454cf669e4d6a85bd58aabe635b54",
            "acdbcfad927548348fae27e7b9202a2f",
            "89c8fb4a1cd24de199a231aebc933c13",
            "def8b455a4e6425d8d3ee70c8b0ec923",
            "030f05a0531b4143a73ff8e550cea9fd",
            "17d7a799ced64f9e93d46d3d2de74cdc",
            "2e62f721674042aa986d3e95a03a34ea"
          ]
        },
        "id": "Tg__QfE4er8H",
        "outputId": "11ef5a67-bd84-4ba7-ec19-acd85fba62c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on 2621440 new tokens...\n",
            "512 steps in total will be taken.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b652a7a69384d15bfc8285ecf68addd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4096 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss at step 1: 1.8202431052923203\n",
            "Loss at step 2: 1.594724103808403\n",
            "Loss at step 3: 1.7100998312234879\n",
            "Loss at step 4: 1.5239116176962852\n",
            "Loss at step 5: 2.1622454226017\n",
            "Loss at step 6: 1.395967721939087\n",
            "Loss at step 7: 1.869606301188469\n",
            "Loss at step 8: 1.5405991077423096\n",
            "Loss at step 9: 1.4592493548989296\n",
            "Loss at step 10: 1.5880023539066315\n",
            "Loss at step 11: 1.3451108038425446\n",
            "Loss at step 12: 1.569413274526596\n",
            "Loss at step 13: 1.414364330470562\n",
            "Loss at step 14: 1.5175216495990753\n",
            "Loss at step 15: 1.469494417309761\n",
            "Loss at step 16: 1.65050208568573\n",
            "Loss at step 17: 1.4970856755971909\n",
            "Loss at step 18: 1.4708647578954697\n",
            "Loss at step 19: 1.3546412885189056\n",
            "Loss at step 20: 1.5448588281869888\n",
            "Loss at step 21: 1.4967026561498642\n",
            "Loss at step 22: 1.3583641350269318\n",
            "Loss at step 23: 1.5146870911121368\n",
            "Loss at step 24: 1.8288955390453339\n",
            "Loss at step 25: 1.3248411118984222\n",
            "Loss at step 26: 1.7258609533309937\n",
            "Loss at step 27: 1.2672379463911057\n",
            "Loss at step 28: 1.4730567187070847\n",
            "Loss at step 29: 1.5074560716748238\n",
            "Loss at step 30: 1.4226033240556717\n",
            "Loss at step 31: 1.4365296438336372\n",
            "Loss at step 32: 1.5845347121357918\n",
            "Loss at step 33: 1.555768221616745\n",
            "Loss at step 34: 1.933328878134489\n",
            "Loss at step 35: 1.651916965842247\n",
            "Loss at step 36: 1.587699979543686\n",
            "Loss at step 37: 1.6340671479701996\n",
            "Loss at step 38: 1.704507052898407\n",
            "Loss at step 39: 1.1414405032992363\n",
            "Loss at step 40: 1.6004297211766243\n",
            "Loss at step 41: 1.3818285912275314\n",
            "Loss at step 42: 1.5597119554877281\n",
            "Loss at step 43: 1.247350126504898\n",
            "Loss at step 44: 1.5320199579000473\n",
            "Loss at step 45: 1.1887640282511711\n",
            "Loss at step 46: 1.8209088668227196\n",
            "Loss at step 47: 1.5887553617358208\n",
            "Loss at step 48: 1.5196435377001762\n",
            "Loss at step 49: 1.4482995569705963\n",
            "Loss at step 50: 1.845612272620201\n",
            "Loss at step 51: 1.6937857568264008\n",
            "Loss at step 52: 1.5611824989318848\n",
            "Loss at step 53: 1.4368868172168732\n",
            "Loss at step 54: 1.5984253361821175\n",
            "Loss at step 55: 1.6371926218271255\n",
            "Loss at step 56: 1.361405923962593\n",
            "Loss at step 57: 1.386246033012867\n",
            "Loss at step 58: 1.0997751131653786\n",
            "Loss at step 59: 1.5647305250167847\n",
            "Loss at step 60: 1.5435403138399124\n",
            "Loss at step 61: 1.8894504755735397\n",
            "Loss at step 62: 1.2720438241958618\n",
            "Loss at step 63: 1.497837871313095\n",
            "Loss at step 64: 1.3572262674570084\n",
            "Loss at step 65: 1.347044676542282\n",
            "Loss at step 66: 1.5634915083646774\n",
            "Loss at step 67: 1.4487357139587402\n",
            "Loss at step 68: 1.2052080035209656\n",
            "Loss at step 69: 1.487841784954071\n",
            "Loss at step 70: 1.9595654681324959\n",
            "Loss at step 71: 1.385779358446598\n",
            "Loss at step 72: 1.7438519299030304\n",
            "Loss at step 73: 1.3962017595767975\n",
            "Loss at step 74: 1.1480744555592537\n",
            "Loss at step 75: 1.2664159536361694\n",
            "Loss at step 76: 1.2742558009922504\n",
            "Loss at step 77: 1.227829746901989\n",
            "Loss at step 78: 1.3730185255408287\n",
            "Loss at step 79: 1.5134020298719406\n",
            "Loss at step 80: 1.2223934978246689\n",
            "Loss at step 81: 1.2619591727852821\n",
            "Loss at step 82: 1.7067727074027061\n",
            "Loss at step 83: 1.3133224546909332\n",
            "Loss at step 84: 1.3279627189040184\n",
            "Loss at step 85: 1.4182523488998413\n",
            "Loss at step 86: 1.3117393627762794\n",
            "Loss at step 87: 1.7698927521705627\n",
            "Loss at step 88: 1.7559839338064194\n",
            "Loss at step 89: 1.6547920852899551\n",
            "Loss at step 90: 1.2813626825809479\n",
            "Loss at step 91: 1.333794541656971\n",
            "Loss at step 92: 1.3998281955718994\n",
            "Loss at step 93: 1.356436625123024\n",
            "Loss at step 94: 1.396937146782875\n",
            "Loss at step 95: 1.5011582672595978\n",
            "Loss at step 96: 1.1774808168411255\n",
            "Loss at step 97: 1.3256521448493004\n",
            "Loss at step 98: 1.2549328282475471\n",
            "Loss at step 99: 1.198574960231781\n",
            "Loss at step 100: 1.5356231778860092\n",
            "Loss at step 101: 1.4275362268090248\n",
            "Loss at step 102: 1.4813897162675858\n",
            "Loss at step 103: 1.6525143831968307\n",
            "Loss at step 104: 1.5052960738539696\n",
            "Loss at step 105: 1.2838386818766594\n",
            "Loss at step 106: 1.249997340142727\n",
            "Loss at step 107: 1.3219851851463318\n",
            "Loss at step 108: 1.456690564751625\n",
            "Loss at step 109: 1.6214830577373505\n",
            "Loss at step 110: 1.3650163561105728\n",
            "Loss at step 111: 1.300179846584797\n",
            "Loss at step 112: 1.6434506103396416\n",
            "Loss at step 113: 1.3609067276120186\n",
            "Loss at step 114: 1.742098368704319\n",
            "Loss at step 115: 1.3789036870002747\n",
            "Loss at step 116: 1.4302285015583038\n",
            "Loss at step 117: 1.7011343836784363\n",
            "Loss at step 118: 2.1475499719381332\n",
            "Loss at step 119: 1.3706629872322083\n",
            "Loss at step 120: 1.3326826021075249\n",
            "Loss at step 121: 1.3039952367544174\n",
            "Loss at step 122: 1.3373408690094948\n",
            "Loss at step 123: 1.0699376165866852\n",
            "Loss at step 124: 1.411337673664093\n",
            "Loss at step 125: 1.2762659043073654\n",
            "Loss at step 126: 1.6994315534830093\n",
            "Loss at step 127: 1.3233637288212776\n",
            "Loss at step 128: 1.3785615041851997\n",
            "Loss at step 129: 1.202193483710289\n",
            "Loss at step 130: 1.2498744279146194\n",
            "Loss at step 131: 1.576578937470913\n",
            "Loss at step 132: 1.2559886425733566\n",
            "Loss at step 133: 1.454853393137455\n",
            "Loss at step 134: 1.2311147078871727\n",
            "Loss at step 135: 1.39551942050457\n",
            "Loss at step 136: 1.4168927371501923\n",
            "Loss at step 137: 1.2465011924505234\n",
            "Loss at step 138: 1.378458857536316\n",
            "Loss at step 139: 1.6007558852434158\n",
            "Loss at step 140: 1.233441337943077\n",
            "Loss at step 141: 1.8190892189741135\n",
            "Loss at step 142: 1.6757135540246964\n",
            "Loss at step 143: 1.302134484052658\n",
            "Loss at step 144: 1.456400290131569\n",
            "Loss at step 145: 1.335949346423149\n",
            "Loss at step 146: 1.417407512664795\n",
            "Loss at step 147: 1.6191974505782127\n",
            "Loss at step 148: 1.5878207311034203\n",
            "Loss at step 149: 1.153784692287445\n",
            "Loss at step 150: 1.3398059532046318\n",
            "Loss at step 151: 1.5080097615718842\n",
            "Loss at step 152: 1.4870974868535995\n",
            "Loss at step 153: 1.834897205233574\n",
            "Loss at step 154: 1.2245257794857025\n",
            "Loss at step 155: 1.7864713072776794\n",
            "Loss at step 156: 1.4326064810156822\n",
            "Loss at step 157: 1.3257834687829018\n",
            "Loss at step 158: 1.3243175074458122\n",
            "Loss at step 159: 1.5256085842847824\n",
            "Loss at step 160: 1.4523434713482857\n",
            "Loss at step 161: 1.229194514453411\n",
            "Loss at step 162: 1.1916944459080696\n",
            "Loss at step 163: 1.3035982176661491\n",
            "Loss at step 164: 1.5250191986560822\n",
            "Loss at step 165: 1.4434951022267342\n",
            "Loss at step 166: 1.8310069739818573\n",
            "Loss at step 167: 1.2907294631004333\n",
            "Loss at step 168: 1.3938754349946976\n",
            "Loss at step 169: 1.5544214397668839\n",
            "Loss at step 170: 1.431264191865921\n",
            "Loss at step 171: 1.8018487840890884\n",
            "Loss at step 172: 1.134629089385271\n",
            "Loss at step 173: 1.4127528369426727\n",
            "Loss at step 174: 1.3425173461437225\n",
            "Loss at step 175: 1.3709765002131462\n",
            "Loss at step 176: 1.1610404402017593\n",
            "Loss at step 177: 1.3734121397137642\n",
            "Loss at step 178: 1.4378068149089813\n",
            "Loss at step 179: 1.566055454313755\n",
            "Loss at step 180: 1.603946678340435\n",
            "Loss at step 181: 1.362822413444519\n",
            "Loss at step 182: 1.4423355013132095\n",
            "Loss at step 183: 1.2895151749253273\n",
            "Loss at step 184: 1.2525951191782951\n",
            "Loss at step 185: 1.1421485617756844\n",
            "Loss at step 186: 1.5473395735025406\n",
            "Loss at step 187: 1.4150592684745789\n",
            "Loss at step 188: 1.3239091783761978\n",
            "Loss at step 189: 1.5617532432079315\n",
            "Loss at step 190: 1.5060252621769905\n",
            "Loss at step 191: 1.3636808842420578\n",
            "Loss at step 192: 1.3153766989707947\n",
            "Loss at step 193: 1.527047075331211\n",
            "Loss at step 194: 1.591720212250948\n",
            "Loss at step 195: 1.150015514343977\n",
            "Loss at step 196: 1.8707644939422607\n",
            "Loss at step 197: 1.2297806292772293\n",
            "Loss at step 198: 1.3814973086118698\n",
            "Loss at step 199: 1.2586965039372444\n",
            "Loss at step 200: 1.1644938215613365\n",
            "Loss at step 201: 1.2536889240145683\n",
            "Loss at step 202: 1.4230096191167831\n",
            "Loss at step 203: 1.3696679770946503\n",
            "Loss at step 204: 1.3465572223067284\n",
            "Loss at step 205: 1.7811442613601685\n",
            "Loss at step 206: 1.1858590096235275\n",
            "Loss at step 207: 1.2015425339341164\n",
            "Loss at step 208: 1.3128190264105797\n",
            "Loss at step 209: 1.3534801602363586\n",
            "Loss at step 210: 1.1604051440954208\n",
            "Loss at step 211: 1.418104164302349\n",
            "Loss at step 212: 1.505680501461029\n",
            "Loss at step 213: 1.3897786140441895\n",
            "Loss at step 214: 1.2717341892421246\n",
            "Loss at step 215: 1.47333774715662\n",
            "Loss at step 216: 1.5285681039094925\n",
            "Loss at step 217: 1.1974982544779778\n",
            "Loss at step 218: 1.4227739721536636\n",
            "Loss at step 219: 1.4629487097263336\n",
            "Loss at step 220: 1.1902923583984375\n",
            "Loss at step 221: 1.0773131251335144\n",
            "Loss at step 222: 1.2599246576428413\n",
            "Loss at step 223: 1.1257058680057526\n",
            "Loss at step 224: 1.4910027012228966\n",
            "Loss at step 225: 1.2450190670788288\n",
            "Loss at step 226: 1.3744792491197586\n",
            "Loss at step 227: 1.234002523124218\n",
            "Loss at step 228: 1.1184879913926125\n",
            "Loss at step 229: 1.376225806772709\n",
            "Loss at step 230: 1.1511348113417625\n",
            "Loss at step 231: 1.4160052090883255\n",
            "Loss at step 232: 1.339160293340683\n",
            "Loss at step 233: 1.318295679986477\n",
            "Loss at step 234: 1.6403800547122955\n",
            "Loss at step 235: 1.3685300201177597\n",
            "Loss at step 236: 1.3166099525988102\n",
            "Loss at step 237: 1.2951167896389961\n",
            "Loss at step 238: 1.289001204073429\n",
            "Loss at step 239: 1.3611226230859756\n",
            "Loss at step 240: 1.3834471926093102\n",
            "Loss at step 241: 1.5367939919233322\n",
            "Loss at step 242: 1.1846846416592598\n",
            "Loss at step 243: 1.4717572778463364\n",
            "Loss at step 244: 1.451171800494194\n",
            "Loss at step 245: 1.3288615867495537\n",
            "Loss at step 246: 1.2408710345625877\n",
            "Loss at step 247: 1.5186590105295181\n",
            "Loss at step 248: 1.1856901794672012\n",
            "Loss at step 249: 1.4037777408957481\n",
            "Loss at step 250: 1.4276249520480633\n",
            "Loss at step 251: 1.5340027958154678\n",
            "Loss at step 252: 1.3135416209697723\n",
            "Loss at step 253: 1.3734062910079956\n",
            "Loss at step 254: 1.2016133964061737\n",
            "Loss at step 255: 1.2276810482144356\n",
            "Loss at step 256: 1.3484156280755997\n",
            "Loss at step 257: 1.203021451830864\n",
            "Loss at step 258: 1.4582720696926117\n",
            "Loss at step 259: 1.4487244784832\n",
            "Loss at step 260: 1.2868371456861496\n",
            "Loss at step 261: 1.509293407201767\n",
            "Loss at step 262: 1.0407122373580933\n",
            "Loss at step 263: 1.3014177307486534\n",
            "Loss at step 264: 1.3614429011940956\n",
            "Loss at step 265: 1.2068877890706062\n",
            "Loss at step 266: 1.4604521840810776\n",
            "Loss at step 267: 1.30824763327837\n",
            "Loss at step 268: 1.2181658744812012\n",
            "Loss at step 269: 1.2723842486739159\n",
            "Loss at step 270: 1.3917559012770653\n",
            "Loss at step 271: 1.2820195406675339\n",
            "Loss at step 272: 1.4321713596582413\n",
            "Loss at step 273: 1.3642805814743042\n",
            "Loss at step 274: 1.6092334687709808\n",
            "Loss at step 275: 1.2921065725386143\n",
            "Loss at step 276: 1.485416665673256\n",
            "Loss at step 277: 1.4338056817650795\n",
            "Loss at step 278: 1.5959154069423676\n",
            "Loss at step 279: 1.1380103826522827\n",
            "Loss at step 280: 1.3195404335856438\n",
            "Loss at step 281: 1.562755011022091\n",
            "Loss at step 282: 1.419564425945282\n",
            "Loss at step 283: 1.1195248514413834\n",
            "Loss at step 284: 1.1453040689229965\n",
            "Loss at step 285: 1.3297196328639984\n",
            "Loss at step 286: 1.432606354355812\n",
            "Loss at step 287: 1.5263036340475082\n",
            "Loss at step 288: 1.2009205743670464\n",
            "Loss at step 289: 1.6477311104536057\n",
            "Loss at step 290: 1.4924275875091553\n",
            "Loss at step 291: 1.3019137606024742\n",
            "Loss at step 292: 1.0113119408488274\n",
            "Loss at step 293: 1.355881616473198\n",
            "Loss at step 294: 1.645551711320877\n",
            "Loss at step 295: 1.4158457219600677\n",
            "Loss at step 296: 1.7140080779790878\n",
            "Loss at step 297: 1.442112736403942\n",
            "Loss at step 298: 1.1515895053744316\n",
            "Loss at step 299: 1.4680248945951462\n",
            "Loss at step 300: 1.6434688866138458\n",
            "Loss at step 301: 1.307333454489708\n",
            "Loss at step 302: 1.7281772792339325\n",
            "Loss at step 303: 1.4077365100383759\n",
            "Loss at step 304: 1.6904982775449753\n",
            "Loss at step 305: 1.4344781637191772\n",
            "Loss at step 306: 1.3628626391291618\n",
            "Loss at step 307: 1.468226432800293\n",
            "Loss at step 308: 1.3664415031671524\n",
            "Loss at step 309: 1.6336558908224106\n",
            "Loss at step 310: 0.8982392828911543\n",
            "Loss at step 311: 1.3586756512522697\n",
            "Loss at step 312: 1.6365474611520767\n",
            "Loss at step 313: 1.495236411690712\n",
            "Loss at step 314: 1.1231132298707962\n",
            "Loss at step 315: 1.528743788599968\n",
            "Loss at step 316: 1.3546785190701485\n",
            "Loss at step 317: 1.270732056349516\n",
            "Loss at step 318: 1.4024858623743057\n",
            "Loss at step 319: 1.3371507972478867\n",
            "Loss at step 320: 0.993566483259201\n",
            "Loss at step 321: 1.0981979966163635\n",
            "Loss at step 322: 1.1956110000610352\n",
            "Loss at step 323: 1.3716566935181618\n",
            "Loss at step 324: 1.1201663240790367\n",
            "Loss at step 325: 1.3373151496052742\n",
            "Loss at step 326: 1.2088980823755264\n",
            "Loss at step 327: 1.1894476786255836\n",
            "Loss at step 328: 1.3931855335831642\n",
            "Loss at step 329: 1.370587982237339\n",
            "Loss at step 330: 1.7116802856326103\n",
            "Loss at step 331: 1.2451522052288055\n",
            "Loss at step 332: 1.2448896914720535\n",
            "Loss at step 333: 1.582958273589611\n",
            "Loss at step 334: 1.2708877250552177\n",
            "Loss at step 335: 1.4644632562994957\n",
            "Loss at step 336: 1.5220141857862473\n",
            "Loss at step 337: 1.3480460867285728\n",
            "Loss at step 338: 1.343762330710888\n",
            "Loss at step 339: 1.10558021068573\n",
            "Loss at step 340: 1.2180252596735954\n",
            "Loss at step 341: 1.581932008266449\n",
            "Loss at step 342: 1.30461485683918\n",
            "Loss at step 343: 1.3884997814893723\n",
            "Loss at step 344: 1.2449504882097244\n",
            "Loss at step 345: 1.195279911160469\n",
            "Loss at step 346: 1.4394426345825195\n",
            "Loss at step 347: 1.3612499460577965\n",
            "Loss at step 348: 1.1907207667827606\n",
            "Loss at step 349: 1.5256712585687637\n",
            "Loss at step 350: 1.3365999609231949\n",
            "Loss at step 351: 1.5280693471431732\n",
            "Loss at step 352: 1.4635715708136559\n",
            "Loss at step 353: 1.1329087540507317\n",
            "Loss at step 354: 1.2496010959148407\n",
            "Loss at step 355: 1.5358348712325096\n",
            "Loss at step 356: 1.5242172554135323\n",
            "Loss at step 357: 1.6288170665502548\n",
            "Loss at step 358: 1.348313644528389\n",
            "Loss at step 359: 1.3506864681839943\n",
            "Loss at step 360: 1.168643094599247\n",
            "Loss at step 361: 1.2904796749353409\n",
            "Loss at step 362: 1.168570563197136\n",
            "Loss at step 363: 1.4165552482008934\n",
            "Loss at step 364: 1.184206873178482\n",
            "Loss at step 365: 1.2279557138681412\n",
            "Loss at step 366: 1.2648624554276466\n",
            "Loss at step 367: 1.2609733045101166\n",
            "Loss at step 368: 1.2520925104618073\n",
            "Loss at step 369: 1.4034461453557014\n",
            "Loss at step 370: 1.555635116994381\n",
            "Loss at step 371: 1.0654594525694847\n",
            "Loss at step 372: 1.1004146188497543\n",
            "Loss at step 373: 1.254906676709652\n",
            "Loss at step 374: 1.0413154810667038\n",
            "Loss at step 375: 1.1967737302184105\n",
            "Loss at step 376: 1.7244433611631393\n",
            "Loss at step 377: 1.4932623505592346\n",
            "Loss at step 378: 1.6508451104164124\n",
            "Loss at step 379: 1.068095549941063\n",
            "Loss at step 380: 1.1882297322154045\n",
            "Loss at step 381: 1.177704095840454\n",
            "Loss at step 382: 1.4154198467731476\n",
            "Loss at step 383: 1.0795451253652573\n",
            "Loss at step 384: 1.2699229717254639\n",
            "Loss at step 385: 1.3411302268505096\n",
            "Loss at step 386: 1.331581462174654\n",
            "Loss at step 387: 1.4212231636047363\n",
            "Loss at step 388: 1.4949243515729904\n",
            "Loss at step 389: 1.4270659163594246\n",
            "Loss at step 390: 1.274396851658821\n",
            "Loss at step 391: 1.4501738473773003\n",
            "Loss at step 392: 1.5498677641153336\n",
            "Loss at step 393: 1.4003691971302032\n",
            "Loss at step 394: 1.564316712319851\n",
            "Loss at step 395: 1.2646473795175552\n",
            "Loss at step 396: 1.2883956730365753\n",
            "Loss at step 397: 1.1532773301005363\n",
            "Loss at step 398: 1.1708188876509666\n",
            "Loss at step 399: 1.582351803779602\n",
            "Loss at step 400: 1.1610211580991745\n",
            "Loss at step 401: 1.188880942761898\n",
            "Loss at step 402: 1.0486908107995987\n",
            "Loss at step 403: 1.1610625386238098\n",
            "Loss at step 404: 1.396957904100418\n",
            "Loss at step 405: 1.4637387096881866\n",
            "Loss at step 406: 1.130483016371727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.DataFrame(losses)\n",
        "plt.plot(\n",
        "    df.ewm(alpha=0.01).mean()\n",
        ")"
      ],
      "metadata": {
        "id": "0DQFSiKmwkq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the (very small, comparative to the model) trained weights\n",
        "# you can share these on the huggingface hub, along with the params for your replace_all_matching_layers step so people can load the model properly\n",
        "save_lora_layers(gpt, \"lora-gpt2-xl-attn-codeparrot.pt\")"
      ],
      "metadata": {
        "id": "DOUiXAPNe0Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test generation\n",
        "prompt = tokenizer(\"def fibonacci(digits=10):\", return_tensors='pt')\n",
        "prompt = {key: value.to(device) for key, value in prompt.items()}\n",
        "with torch.no_grad():\n",
        "    out = gpt.generate(**prompt, min_length=64, max_length=64, do_sample=True)\n",
        "print(tokenizer.decode(out[0]))"
      ],
      "metadata": {
        "id": "VbL0TUuDzeoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's how to load the params into a model once you've trained.\n",
        "```python \n",
        "model = # whatever\n",
        "matches = [\"attn\"] \n",
        "replace_all_matching_layers(model, r=16, matches=matches)\n",
        "load_lora_layers(model, \"lora-gpt2-xl-attn-codeparrot.pt\")\n",
        "```"
      ],
      "metadata": {
        "id": "rOz50fQdccEa"
      }
    }
  ]
}